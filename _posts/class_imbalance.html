---
layout: post
excerpt: An introduction to Reinforcement Learning and a look of two of the most important papers
permalink: /Deep-Learning-Research-Review-Week-2-Reinforcement-Learning
images:
  - url: /assets/Cover6th.png
---
<p><em>This is the 2<sup>nd</sup> installment of a new series called Deep Learning Research Review. Every couple weeks or so, I&rsquo;ll be summarizing and explaining research papers in specific subfields of deep learning. This week focuses on Reinforcement Learning. </em><a href="https://adeshpande3.github.io/adeshpande3.github.io/Deep-Learning-Research-Review-Week-1-Generative-Adversarial-Nets"  target="_blank"><em>Last time</em></a><em> was Generative Adversarial Networks ICYMI</em></p>
<h2><strong>Introduction to Reinforcement Learning</strong></h2>
<p><span style="text-decoration: underline;">3 Categories of Machine Learning</span></p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Before getting into the papers, let&rsquo;s first talk about what <strong>reinforcement learning</strong> is. The field of machine learning can be separated into 3 main categories.</p>
<ol>
<li>Supervised Learning</li>
<li>Unsupervised Learning</li>
<li>Reinforcement Learning</li>
</ol>

<p>The first category, <strong>supervised learning</strong>, is the one you may be most familiar with. It relies on the idea of creating a function or model based on a set of training data, which contains inputs and their corresponding labels. Convolutional Neural Networks are a great example of this, as the images are the inputs and the outputs are the classifications of the images (dog, cat, etc).</p>
<p><strong>Unsupervised learning</strong> seeks to find some sort of structure within data through methods of cluster analysis. One of the most well-known ML clustering algorithms, K-Means, is an example of unsupervised learning.</p>
<p><strong>Reinforcement learning</strong> is the task of learning what actions to take, given a certain situation/environment, so as to maximize a reward signal. The interesting difference between supervised and reinforcement learning is that this reward signal simply tells you whether the action (or input) that the agent takes is good or bad. It doesn&rsquo;t tell you anything about what the <em>best</em> action is. Contrast this to CNNs where the corresponding label for each image input is a definite instruction of what the output should be for each input.&nbsp; Another unique component of RL is that an agent&rsquo;s actions will affect the subsequent data it receives. For example, an agent&rsquo;s action of moving left instead of right means that the agent will receive different input from the environment at the next time step. Let&rsquo;s look at an example to start off.</p>
<p><span style="text-decoration: underline;">The RL Problem</span></p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; So, let&rsquo;s first think about what have in a reinforcement learning problem. Let&rsquo;s imagine a tiny robot in a small room. We haven&rsquo;t programmed this robot to move or walk or take any action. It&rsquo;s just standing there. This robot is our <strong>agent</strong>.</p>

<p>Like we mentioned before, reinforcement learning is all about trying to understand the optimal way of making decisions/actions so that we maximize some <strong>reward</strong> <strong>R</strong>. This reward is a feedback signal that just indicates how well the agent is doing at a given time step. The <strong>action A</strong> that an agent takes at every time step is a function of both the reward (signal telling the agent how well it&rsquo;s currently doing) and the <strong>state S</strong>, which is a description of the environment the agent is in. The mapping from environment states to actions is called our <strong>policy P</strong>. The policy basically defines the agent&rsquo;s way of behaving at a certain time, given a certain situation. Now, we also have a <strong>value function V</strong> which is a measure of how good each position is. This is different from the reward in that the reward signal indicates what is good in the immediate sense, while the value function is more indicative of how good it is to be in this state/position in the long run. Finally, we also have a <strong>model M</strong> which is the agent&rsquo;s representation of the environment. This is the agent&rsquo;s model of how it thinks that the environment is going to behave.</p>


